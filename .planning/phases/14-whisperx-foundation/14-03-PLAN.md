---
phase: 14-whisperx-foundation
plan: 03
type: execute
wave: 3
depends_on: ["14-02"]
files_modified:
  - tests/test_whisperx_wrapper.py
autonomous: true

must_haves:
  truths:
    - "Unit tests verify WhisperXPipeline initialization"
    - "Unit tests verify transcribe_and_diarize() with mocked whisperx"
    - "Unit tests verify error handling (AuthenticationError, DiarizationError)"
    - "Tests run without requiring actual whisperx models"
  artifacts:
    - path: "tests/test_whisperx_wrapper.py"
      provides: "Unit tests for WhisperXPipeline"
      min_lines: 150
  key_links:
    - from: "tests/test_whisperx_wrapper.py"
      to: "cesar/whisperx_wrapper.py"
      via: "import and test"
      pattern: "from cesar\\.whisperx_wrapper import"
---

<objective>
Create comprehensive unit tests for WhisperXPipeline module

Purpose: Verify each pipeline stage works correctly with mocked whisperx to enable fast CI without model downloads. Tests serve as documentation for expected behavior.

Output: tests/test_whisperx_wrapper.py with full coverage of WhisperXPipeline
</objective>

<execution_context>
@/home/buckleyrobinson/.claude/get-shit-done/workflows/execute-plan.md
@/home/buckleyrobinson/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/14-whisperx-foundation/14-RESEARCH.md
@tests/test_diarization.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create unit tests for WhisperXPipeline</name>
  <files>tests/test_whisperx_wrapper.py</files>
  <action>
Create tests/test_whisperx_wrapper.py following the pattern from test_diarization.py:

```python
"""Unit tests for WhisperX wrapper module."""
import unittest
from unittest.mock import Mock, patch, MagicMock
from pathlib import Path
import os

from cesar.whisperx_wrapper import (
    WhisperXPipeline,
    WhisperXSegment,
)
from cesar.diarization import DiarizationError, AuthenticationError


class TestWhisperXSegment(unittest.TestCase):
    """Tests for WhisperXSegment dataclass."""

    def test_segment_creation(self):
        """Test creating a WhisperXSegment."""
        segment = WhisperXSegment(
            start=1.5,
            end=3.7,
            speaker="SPEAKER_00",
            text="Hello world"
        )
        self.assertEqual(segment.start, 1.5)
        self.assertEqual(segment.end, 3.7)
        self.assertEqual(segment.speaker, "SPEAKER_00")
        self.assertEqual(segment.text, "Hello world")

    def test_segment_equality(self):
        """Test segment equality comparison."""
        seg1 = WhisperXSegment(0.0, 1.0, "SPEAKER_00", "test")
        seg2 = WhisperXSegment(0.0, 1.0, "SPEAKER_00", "test")
        self.assertEqual(seg1, seg2)


class TestWhisperXPipelineInit(unittest.TestCase):
    """Tests for WhisperXPipeline initialization."""

    def test_init_defaults(self):
        """Test initialization with defaults."""
        pipeline = WhisperXPipeline()
        self.assertEqual(pipeline.model_name, "large-v2")
        self.assertEqual(pipeline.batch_size, 16)

    def test_init_custom_model(self):
        """Test initialization with custom model."""
        pipeline = WhisperXPipeline(model_name="base")
        self.assertEqual(pipeline.model_name, "base")

    def test_init_with_token(self):
        """Test initialization with provided token."""
        pipeline = WhisperXPipeline(hf_token="test_token")
        self.assertEqual(pipeline.hf_token, "test_token")

    @patch.dict(os.environ, {"HF_TOKEN": "env_token"})
    def test_token_from_environment(self):
        """Test token resolution from environment variable."""
        pipeline = WhisperXPipeline()
        self.assertEqual(pipeline.hf_token, "env_token")

    @patch.dict(os.environ, {}, clear=True)
    def test_token_from_cache(self):
        """Test token resolution from cached file."""
        with patch.object(Path, 'exists', return_value=True):
            with patch.object(Path, 'read_text', return_value="cached_token\n"):
                pipeline = WhisperXPipeline()
                self.assertEqual(pipeline.hf_token, "cached_token")

    @patch.dict(os.environ, {}, clear=True)
    def test_token_none_when_not_found(self):
        """Test token is None when not found anywhere."""
        with patch.object(Path, 'exists', return_value=False):
            pipeline = WhisperXPipeline()
            self.assertIsNone(pipeline.hf_token)


class TestWhisperXPipelineDeviceResolution(unittest.TestCase):
    """Tests for device and compute type resolution."""

    def test_device_auto_with_cuda(self):
        """Test auto device selects cuda when available."""
        with patch('torch.cuda.is_available', return_value=True):
            pipeline = WhisperXPipeline(device="auto")
            self.assertEqual(pipeline.device, "cuda")

    def test_device_auto_without_cuda(self):
        """Test auto device selects cpu when cuda unavailable."""
        with patch('torch.cuda.is_available', return_value=False):
            pipeline = WhisperXPipeline(device="auto")
            self.assertEqual(pipeline.device, "cpu")

    def test_device_explicit_cpu(self):
        """Test explicit cpu device."""
        pipeline = WhisperXPipeline(device="cpu")
        self.assertEqual(pipeline.device, "cpu")

    def test_compute_type_auto_cuda(self):
        """Test auto compute type for cuda."""
        with patch('torch.cuda.is_available', return_value=True):
            pipeline = WhisperXPipeline(device="cuda", compute_type="auto")
            self.assertEqual(pipeline.compute_type, "float16")

    def test_compute_type_auto_cpu(self):
        """Test auto compute type for cpu."""
        pipeline = WhisperXPipeline(device="cpu", compute_type="auto")
        self.assertEqual(pipeline.compute_type, "int8")


class TestWhisperXPipelineTranscription(unittest.TestCase):
    """Tests for transcribe_and_diarize method."""

    def _create_mock_whisperx(self):
        """Create mocked whisperx module."""
        mock_whisperx = Mock()

        # Mock load_audio
        mock_whisperx.load_audio.return_value = Mock()

        # Mock load_model
        mock_model = Mock()
        mock_model.transcribe.return_value = {
            "segments": [
                {"text": "Hello world", "start": 0.0, "end": 2.5}
            ],
            "language": "en"
        }
        mock_whisperx.load_model.return_value = mock_model

        # Mock load_align_model
        mock_align_model = Mock()
        mock_align_metadata = Mock()
        mock_whisperx.load_align_model.return_value = (mock_align_model, mock_align_metadata)

        # Mock align
        mock_whisperx.align.return_value = {
            "segments": [
                {
                    "text": "Hello world",
                    "start": 0.0,
                    "end": 2.5,
                    "words": [
                        {"word": "Hello", "start": 0.0, "end": 1.0},
                        {"word": "world", "start": 1.2, "end": 2.5}
                    ]
                }
            ]
        }

        # Mock DiarizationPipeline
        mock_diarize_result = Mock()
        mock_diarize_pipeline = Mock(return_value=mock_diarize_result)
        mock_whisperx.DiarizationPipeline.return_value = mock_diarize_pipeline

        # Mock assign_word_speakers
        mock_whisperx.assign_word_speakers.return_value = {
            "segments": [
                {
                    "text": "Hello world",
                    "start": 0.0,
                    "end": 2.5,
                    "speaker": "SPEAKER_00"
                }
            ]
        }

        return mock_whisperx

    def test_transcribe_and_diarize_success(self):
        """Test successful transcription and diarization."""
        mock_whisperx = self._create_mock_whisperx()

        with patch.dict('sys.modules', {'whisperx': mock_whisperx}):
            with patch('torch.cuda.is_available', return_value=False):
                pipeline = WhisperXPipeline(hf_token="token", device="cpu")
                segments, speaker_count, duration = pipeline.transcribe_and_diarize(
                    "test.wav"
                )

        self.assertEqual(len(segments), 1)
        self.assertEqual(segments[0].text, "Hello world")
        self.assertEqual(segments[0].speaker, "SPEAKER_00")
        self.assertEqual(segments[0].start, 0.0)
        self.assertEqual(segments[0].end, 2.5)
        self.assertEqual(speaker_count, 1)

    def test_transcribe_and_diarize_multiple_speakers(self):
        """Test with multiple speakers."""
        mock_whisperx = self._create_mock_whisperx()

        # Update mock for multiple speakers
        mock_whisperx.assign_word_speakers.return_value = {
            "segments": [
                {"text": "Hello", "start": 0.0, "end": 1.0, "speaker": "SPEAKER_00"},
                {"text": "Hi there", "start": 1.5, "end": 3.0, "speaker": "SPEAKER_01"},
                {"text": "How are you", "start": 3.5, "end": 5.0, "speaker": "SPEAKER_00"}
            ]
        }

        with patch.dict('sys.modules', {'whisperx': mock_whisperx}):
            with patch('torch.cuda.is_available', return_value=False):
                pipeline = WhisperXPipeline(hf_token="token", device="cpu")
                segments, speaker_count, duration = pipeline.transcribe_and_diarize(
                    "test.wav"
                )

        self.assertEqual(len(segments), 3)
        self.assertEqual(speaker_count, 2)
        self.assertEqual(segments[0].speaker, "SPEAKER_00")
        self.assertEqual(segments[1].speaker, "SPEAKER_01")

    def test_transcribe_with_speaker_hints(self):
        """Test min/max speaker hints are passed to diarization."""
        mock_whisperx = self._create_mock_whisperx()

        with patch.dict('sys.modules', {'whisperx': mock_whisperx}):
            with patch('torch.cuda.is_available', return_value=False):
                pipeline = WhisperXPipeline(hf_token="token", device="cpu")
                pipeline.transcribe_and_diarize(
                    "test.wav",
                    min_speakers=2,
                    max_speakers=4
                )

        # Verify diarization was called with speaker hints
        mock_diarize = mock_whisperx.DiarizationPipeline.return_value
        mock_diarize.assert_called_once()
        call_kwargs = mock_diarize.call_args
        # The call should include min_speakers and max_speakers
        # Exact verification depends on implementation

    def test_progress_callback(self):
        """Test progress callback is called during pipeline."""
        mock_whisperx = self._create_mock_whisperx()
        progress_calls = []

        def progress_callback(phase, pct):
            progress_calls.append((phase, pct))

        with patch.dict('sys.modules', {'whisperx': mock_whisperx}):
            with patch('torch.cuda.is_available', return_value=False):
                pipeline = WhisperXPipeline(hf_token="token", device="cpu")
                pipeline.transcribe_and_diarize(
                    "test.wav",
                    progress_callback=progress_callback
                )

        # Verify progress was reported
        self.assertGreater(len(progress_calls), 0)
        phases = [call[0] for call in progress_calls]
        self.assertTrue(any("Transcrib" in p for p in phases))


class TestWhisperXPipelineErrors(unittest.TestCase):
    """Tests for error handling."""

    def test_authentication_error(self):
        """Test AuthenticationError on 401."""
        mock_whisperx = Mock()
        mock_whisperx.load_audio.return_value = Mock()

        mock_model = Mock()
        mock_model.transcribe.return_value = {"segments": [], "language": "en"}
        mock_whisperx.load_model.return_value = mock_model

        mock_whisperx.load_align_model.return_value = (Mock(), Mock())
        mock_whisperx.align.return_value = {"segments": []}

        # Make DiarizationPipeline raise auth error
        mock_whisperx.DiarizationPipeline.side_effect = Exception("401 Unauthorized")

        with patch.dict('sys.modules', {'whisperx': mock_whisperx}):
            with patch('torch.cuda.is_available', return_value=False):
                pipeline = WhisperXPipeline(hf_token="bad_token", device="cpu")
                with self.assertRaises(AuthenticationError) as ctx:
                    pipeline.transcribe_and_diarize("test.wav")

        self.assertIn("authentication", str(ctx.exception).lower())

    def test_diarization_error_generic(self):
        """Test DiarizationError on generic failure."""
        mock_whisperx = Mock()
        mock_whisperx.load_audio.side_effect = Exception("File not found")

        with patch.dict('sys.modules', {'whisperx': mock_whisperx}):
            with patch('torch.cuda.is_available', return_value=False):
                pipeline = WhisperXPipeline(hf_token="token", device="cpu")
                with self.assertRaises(DiarizationError):
                    pipeline.transcribe_and_diarize("nonexistent.wav")


class TestWhisperXPipelineLazyLoading(unittest.TestCase):
    """Tests for lazy model loading."""

    def test_models_not_loaded_on_init(self):
        """Test models are not loaded during __init__."""
        with patch('torch.cuda.is_available', return_value=False):
            pipeline = WhisperXPipeline(hf_token="token")

        # Models should be None until first use
        self.assertIsNone(pipeline._whisper_model)
        self.assertIsNone(pipeline._align_model)
        self.assertIsNone(pipeline._diarize_model)


if __name__ == "__main__":
    unittest.main()
```

The tests cover:
1. **WhisperXSegment** - dataclass creation and equality
2. **Initialization** - defaults, custom values, token resolution
3. **Device resolution** - auto/cuda/cpu selection
4. **Compute type** - auto selection based on device
5. **Transcription** - full pipeline with mocked whisperx
6. **Multiple speakers** - speaker counting
7. **Speaker hints** - min/max_speakers parameters
8. **Progress callback** - callback invocation
9. **Error handling** - AuthenticationError, DiarizationError
10. **Lazy loading** - models not loaded until first use

Use extensive mocking to avoid requiring actual whisperx models for CI.
  </action>
  <verify>
Run: `python -m pytest tests/test_whisperx_wrapper.py -v`
Expected: All tests pass (may need 14-02 complete first)

Run: `grep -c "def test_" tests/test_whisperx_wrapper.py`
Expected: At least 15 test methods
  </verify>
  <done>
tests/test_whisperx_wrapper.py exists with comprehensive unit tests covering initialization, pipeline execution, error handling, and lazy loading
  </done>
</task>

<task type="auto">
  <name>Task 2: Run tests and verify coverage</name>
  <files>None (verification only)</files>
  <action>
Run the test suite to verify all tests pass:

1. Run whisperx wrapper tests:
```
python -m pytest tests/test_whisperx_wrapper.py -v
```

2. If tests fail due to missing 14-02 implementation:
   - This is expected if running before 14-02 completes
   - Tests should at least import without errors
   - Skip to verification step

3. Run full test suite to check for regressions:
```
python -m pytest tests/ -v --ignore=tests/test_whisperx_wrapper.py
```
   - Existing tests should still pass
   - whisperx_wrapper tests may fail until 14-02 completes

4. Check test count:
```
grep -c "def test_" tests/test_whisperx_wrapper.py
```
   - Should be at least 15 test methods
  </action>
  <verify>
Run: `python -c "import tests.test_whisperx_wrapper; print('Import OK')"`
Expected: Prints "Import OK" (test file is syntactically correct)

Run: `grep -c "def test_" tests/test_whisperx_wrapper.py`
Expected: At least 15 test methods
  </verify>
  <done>
Test file imports without errors and contains at least 15 test methods covering all WhisperXPipeline functionality
  </done>
</task>

</tasks>

<verification>
1. `ls tests/test_whisperx_wrapper.py` confirms file exists
2. `python -c "import tests.test_whisperx_wrapper"` imports without errors
3. `grep -c "def test_" tests/test_whisperx_wrapper.py` shows 15+ test methods
4. Test classes cover: WhisperXSegment, Init, DeviceResolution, Transcription, Errors, LazyLoading
</verification>

<success_criteria>
- tests/test_whisperx_wrapper.py exists with 15+ test methods
- Tests cover initialization, device resolution, pipeline execution
- Tests cover error handling (AuthenticationError, DiarizationError)
- Tests use mocked whisperx to avoid model downloads
- Test file imports without syntax errors
- Tests verify lazy model loading behavior
</success_criteria>

<output>
After completion, create `.planning/phases/14-whisperx-foundation/14-03-SUMMARY.md`
</output>
