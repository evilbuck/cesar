---
phase: 02-foundation
plan: 02
type: execute
wave: 2
depends_on: ["02-01"]
files_modified:
  - cesar/api/database.py
  - cesar/api/repository.py
  - cesar/api/__init__.py
  - tests/test_repository.py
autonomous: true

must_haves:
  truths:
    - "Job can be created and persisted to SQLite"
    - "Job can be retrieved by ID"
    - "Job state transitions update the database"
    - "Jobs survive application restart (SQLite file persistence)"
    - "All jobs can be listed"
    - "Timestamps are stored and retrieved correctly"
  artifacts:
    - path: "cesar/api/database.py"
      provides: "SQLite schema and initialization"
      exports: ["SCHEMA", "initialize_schema"]
      min_lines: 20
    - path: "cesar/api/repository.py"
      provides: "JobRepository with async CRUD operations"
      exports: ["JobRepository"]
      min_lines: 80
    - path: "tests/test_repository.py"
      provides: "Integration tests for repository"
      min_lines: 100
  key_links:
    - from: "cesar/api/repository.py"
      to: "cesar/api/models.py"
      via: "Job model import"
      pattern: "from cesar\\.api\\.models import Job"
    - from: "cesar/api/repository.py"
      to: "aiosqlite"
      via: "async database connection"
      pattern: "await aiosqlite\\.connect"
    - from: "cesar/api/repository.py"
      to: "cesar/api/database.py"
      via: "schema initialization"
      pattern: "initialize_schema"
---

<objective>
Create the SQLite persistence layer with schema initialization and JobRepository for async CRUD operations.

Purpose: Enable job data to be persisted across server restarts. The repository pattern abstracts database operations, making the system testable and maintainable.

Output: `cesar/api/database.py` with schema, `cesar/api/repository.py` with JobRepository class, plus integration tests.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-foundation/02-RESEARCH.md
@.planning/phases/02-foundation/02-01-SUMMARY.md

# Implementation from Plan 01
@cesar/api/models.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create database schema and initialization</name>
  <files>cesar/api/database.py</files>
  <action>
Create `cesar/api/database.py` with:

1. **SCHEMA constant** - SQL for jobs table:
   ```sql
   CREATE TABLE IF NOT EXISTS jobs (
       id TEXT PRIMARY KEY,
       status TEXT NOT NULL DEFAULT 'queued',
       audio_path TEXT NOT NULL,
       model_size TEXT NOT NULL DEFAULT 'base',
       created_at TEXT NOT NULL,
       started_at TEXT,
       completed_at TEXT,
       result_text TEXT,
       detected_language TEXT,
       error_message TEXT
   );

   CREATE INDEX IF NOT EXISTS idx_jobs_status ON jobs(status);
   CREATE INDEX IF NOT EXISTS idx_jobs_created ON jobs(created_at DESC);
   ```

2. **initialize_schema(connection)** async function:
   - Takes aiosqlite.Connection
   - Executes SCHEMA via executescript
   - Commits the transaction

3. **get_default_db_path()** function:
   - Returns Path to `~/.local/share/cesar/jobs.db`
   - Creates parent directory if it doesn't exist
   - Use `pathlib.Path.home()` for cross-platform home directory

Note: Timestamps stored as ISO 8601 TEXT strings per research doc.
  </action>
  <verify>
    python -c "from cesar.api.database import SCHEMA, initialize_schema, get_default_db_path; print(get_default_db_path())"
  </verify>
  <done>
    Database module exports SCHEMA, initialize_schema, and get_default_db_path
  </done>
</task>

<task type="auto">
  <name>Task 2: Create JobRepository with async CRUD</name>
  <files>
    cesar/api/repository.py
    cesar/api/__init__.py
  </files>
  <action>
Create `cesar/api/repository.py` with JobRepository class:

1. **__init__(self, db_path: Path)**
   - Store db_path
   - Initialize _connection to None

2. **async connect(self)**
   - Open connection via aiosqlite.connect(db_path)
   - Set PRAGMAs per research:
     - `PRAGMA journal_mode=WAL;`
     - `PRAGMA busy_timeout=5000;`
     - `PRAGMA foreign_keys=ON;`
     - `PRAGMA synchronous=NORMAL;`
   - Call initialize_schema to create tables
   - Commit

3. **async close(self)**
   - Close connection if open
   - Set _connection to None

4. **async create(self, job: Job) -> Job**
   - INSERT job into jobs table
   - All fields from Job model
   - Convert timestamps to ISO format strings
   - Convert status to .value
   - Commit and return job

5. **async get(self, job_id: str) -> Optional[Job]**
   - SELECT * WHERE id = ?
   - Return None if not found
   - Convert row to Job via _row_to_job helper

6. **async update(self, job: Job) -> Job**
   - UPDATE job by id
   - Update all mutable fields (status, started_at, completed_at, result_text, detected_language, error_message)
   - Commit and return job

7. **async list_all(self) -> List[Job]**
   - SELECT * ORDER BY created_at DESC
   - Convert all rows to Jobs

8. **async get_next_queued(self) -> Optional[Job]**
   - SELECT * WHERE status = 'queued' ORDER BY created_at ASC LIMIT 1
   - Returns oldest queued job (for worker in Phase 3)

9. **_row_to_job(self, row: tuple) -> Job**
   - Helper to convert database row to Job model
   - Parse ISO timestamps back to datetime
   - Parse status string to JobStatus enum

Update `cesar/api/__init__.py` to export JobRepository, Job, JobStatus.
  </action>
  <verify>
    python -c "from cesar.api import JobRepository, Job, JobStatus; print('Imports OK')"
  </verify>
  <done>
    JobRepository class complete with all CRUD methods, exports working
  </done>
</task>

<task type="auto">
  <name>Task 3: Create repository integration tests</name>
  <files>tests/test_repository.py</files>
  <action>
Create `tests/test_repository.py` with async tests using unittest and asyncio:

1. **Test setup:**
   - Use in-memory database (`:memory:`) for test isolation
   - Create fresh repository per test via setUp/tearDown
   - Use `asyncio.run()` wrapper or `unittest.IsolatedAsyncioTestCase`

2. **Test create and get:**
   - Create job with audio_path
   - Verify job.id is set
   - Get job by id
   - Verify all fields match

3. **Test update (state transitions):**
   - Create job (queued)
   - Update to processing with started_at
   - Verify status and timestamp persisted
   - Update to completed with result_text, detected_language, completed_at
   - Verify all fields persisted

4. **Test error state:**
   - Create job
   - Update to error with error_message
   - Verify error_message persisted

5. **Test list_all:**
   - Create 3 jobs
   - List all
   - Verify count is 3
   - Verify ordered by created_at DESC

6. **Test get_next_queued:**
   - Create 3 jobs in order
   - Get next queued
   - Verify returns oldest (first created)
   - Update first to processing
   - Get next queued
   - Verify returns second created

7. **Test get not found:**
   - Get with non-existent ID
   - Verify returns None

8. **Test persistence (file-based):**
   - Use temp file for database
   - Create job
   - Close connection
   - Reopen connection
   - Verify job still exists
   - Verify timestamps preserved correctly

9. **Test timestamps:**
   - Create job
   - Verify created_at is datetime, not None
   - Update with started_at, completed_at
   - Verify all timestamps are datetime objects after retrieval
  </action>
  <verify>
    python -m pytest tests/test_repository.py -v
  </verify>
  <done>
    All repository tests pass, including persistence and state transition tests
  </done>
</task>

</tasks>

<verification>
Run full test suite:
```bash
python -m pytest tests/ -v
```

Verify end-to-end persistence manually:
```bash
python -c "
import asyncio
from pathlib import Path
import tempfile
from cesar.api import JobRepository, Job, JobStatus
from datetime import datetime

async def test_persistence():
    # Use temp file
    with tempfile.NamedTemporaryFile(suffix='.db', delete=False) as f:
        db_path = Path(f.name)

    # Create and persist job
    repo = JobRepository(db_path)
    await repo.connect()

    job = Job(audio_path='/test/audio.mp3', model_size='small')
    await repo.create(job)
    print(f'Created job: {job.id}')

    # Update to completed
    job.status = JobStatus.COMPLETED
    job.started_at = datetime.utcnow()
    job.completed_at = datetime.utcnow()
    job.result_text = 'Hello world'
    job.detected_language = 'en'
    await repo.update(job)
    print('Updated to completed')

    await repo.close()

    # Reopen and verify
    repo2 = JobRepository(db_path)
    await repo2.connect()

    retrieved = await repo2.get(job.id)
    print(f'Retrieved: status={retrieved.status.value}, text={retrieved.result_text}')

    await repo2.close()

    # Cleanup
    db_path.unlink()
    print('Persistence test passed!')

asyncio.run(test_persistence())
"
```
</verification>

<success_criteria>
- [ ] `cesar/api/database.py` exists with SCHEMA and initialize_schema
- [ ] `cesar/api/repository.py` exists with JobRepository class
- [ ] JobRepository has connect, close, create, get, update, list_all, get_next_queued methods
- [ ] All methods are async
- [ ] Jobs persist to SQLite file (JOB-01, JOB-07)
- [ ] State transitions work (JOB-02)
- [ ] Timestamps preserved correctly (JOB-03)
- [ ] Error messages stored (JOB-04)
- [ ] Result text and language stored (RES-01, RES-02)
- [ ] All tests pass including persistence test
- [ ] No regressions in existing tests
</success_criteria>

<output>
After completion, create `.planning/phases/02-foundation/02-02-SUMMARY.md`
</output>
