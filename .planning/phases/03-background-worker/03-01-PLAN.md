---
phase: 03-background-worker
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - cesar/api/worker.py
  - cesar/api/__init__.py
  - tests/test_worker.py
autonomous: true

must_haves:
  truths:
    - "Multiple jobs can be queued while one is processing"
    - "Jobs process one at a time in FIFO order"
    - "Worker picks up pending jobs automatically"
    - "Worker stops gracefully on shutdown signal"
    - "Failed transcription marks job as ERROR with message"
  artifacts:
    - path: "cesar/api/worker.py"
      provides: "BackgroundWorker class"
      exports: ["BackgroundWorker"]
      min_lines: 100
    - path: "tests/test_worker.py"
      provides: "Unit tests for worker behavior"
      min_lines: 150
  key_links:
    - from: "cesar/api/worker.py"
      to: "cesar/api/repository.py"
      via: "JobRepository dependency injection"
      pattern: "self\\.repository\\.(get_next_queued|update)"
    - from: "cesar/api/worker.py"
      to: "cesar/transcriber.py"
      via: "asyncio.to_thread for blocking transcription"
      pattern: "asyncio\\.to_thread.*transcrib"
---

<objective>
Implement background worker that processes transcription jobs sequentially

Purpose: Enable async job processing where jobs are queued via API and processed in the background one at a time, keeping the event loop responsive.

Output:
- BackgroundWorker class with async run() loop
- Integration with AudioTranscriber via asyncio.to_thread()
- Graceful shutdown handling
- Comprehensive unit tests
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/03-background-worker/03-RESEARCH.md

# Phase 2 artifacts (Job model and repository)
@cesar/api/models.py
@cesar/api/repository.py
@cesar/transcriber.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create BackgroundWorker class</name>
  <files>cesar/api/worker.py, cesar/api/__init__.py</files>
  <action>
Create BackgroundWorker class in cesar/api/worker.py following the research patterns:

1. **Constructor:**
   - `__init__(self, repository: JobRepository, poll_interval: float = 1.0)`
   - Store repository reference
   - Create `_shutdown_event = asyncio.Event()`
   - Track `_current_job_id: Optional[str] = None`

2. **Properties:**
   - `is_processing -> bool`: Returns True if currently processing a job
   - `current_job_id -> Optional[str]`: ID of job being processed

3. **Async run() loop:**
   - Loop while not `_shutdown_event.is_set()`
   - Call `repository.get_next_queued()` to poll for work
   - If no job: `await asyncio.wait_for(_shutdown_event.wait(), timeout=poll_interval)` with TimeoutError handling
   - If job found: call `_process_job(job)`
   - Wrap in try/except to log errors and continue

4. **Async shutdown():**
   - Log shutdown request
   - Set `_shutdown_event`

5. **Async _process_job(job):**
   - Set `_current_job_id = job.id`
   - Update job to PROCESSING with `started_at = datetime.utcnow()`
   - Call `await repository.update(job)`
   - Run transcription via `await asyncio.to_thread(self._run_transcription, job.audio_path, job.model_size)`
   - On success: Update job to COMPLETED with result_text, detected_language, completed_at
   - On exception: Update job to ERROR with error_message, completed_at
   - Finally: Set `_current_job_id = None`

6. **Sync _run_transcription(audio_path, model_size):**
   - Create temp output file with `tempfile.mkstemp(suffix='.txt')`
   - Create `AudioTranscriber(model_size=model_size)`
   - Call `transcriber.transcribe_file(audio_path, output_path)`
   - Read text from output file
   - Clean up temp file in finally block
   - Return `{"text": text, "language": result.get("language", "unknown")}`

Use logging module for all log statements (info for start/stop/complete, error for failures).

Update cesar/api/__init__.py to export BackgroundWorker.
  </action>
  <verify>
Run: `python -c "from cesar.api.worker import BackgroundWorker; print('Import OK')"`
Verify: No import errors
  </verify>
  <done>
BackgroundWorker class exists with run(), shutdown(), and _process_job() methods. Imports successfully.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create worker unit tests</name>
  <files>tests/test_worker.py</files>
  <action>
Create comprehensive unit tests in tests/test_worker.py using unittest.IsolatedAsyncioTestCase:

1. **Setup:**
   - Import BackgroundWorker, Job, JobStatus, JobRepository
   - Use unittest.mock for AsyncMock and patch

2. **Test: test_worker_processes_queued_job**
   - Mock repository with get_next_queued returning job once, then None
   - Mock _run_transcription to return {"text": "Hello", "language": "en"}
   - Start worker task, wait briefly, shutdown, await task
   - Verify repository.update called with PROCESSING then COMPLETED status

3. **Test: test_worker_graceful_shutdown**
   - Mock repository.get_next_queued returning None
   - Start worker task
   - Call shutdown()
   - Verify worker stops within reasonable timeout (1 second)

4. **Test: test_worker_handles_transcription_error**
   - Mock repository with one job
   - Mock _run_transcription to raise FileNotFoundError
   - Run worker briefly
   - Verify job updated to ERROR status with error_message set

5. **Test: test_worker_fifo_order**
   - Create 3 jobs with different created_at timestamps
   - Mock get_next_queued to return them in FIFO order (oldest first)
   - Verify jobs processed in order by checking update call sequence

6. **Test: test_worker_is_processing_property**
   - Mock repository with one job
   - Mock _run_transcription to sleep briefly
   - Check is_processing and current_job_id during processing
   - Verify they reset after completion

7. **Test: test_worker_continues_after_error**
   - First job fails, second succeeds
   - Verify second job still processed (worker doesn't crash)

8. **Test: test_multiple_jobs_queued**
   - Queue 3 jobs
   - Process all sequentially
   - Verify all completed in order

Follow existing test conventions from tests/test_repository.py (IsolatedAsyncioTestCase, setUp/tearDown patterns).
  </action>
  <verify>
Run: `python -m pytest tests/test_worker.py -v`
Expected: All tests pass
  </verify>
  <done>
All worker tests pass. Tests cover: job processing, graceful shutdown, error handling, FIFO order, properties, recovery after error, multiple jobs.
  </done>
</task>

</tasks>

<verification>
1. Import check: `python -c "from cesar.api import BackgroundWorker; print('OK')"`
2. All tests pass: `python -m pytest tests/test_worker.py -v`
3. Full test suite: `python -m pytest tests/ -v` (existing tests still pass)
</verification>

<success_criteria>
- BackgroundWorker class exists in cesar/api/worker.py
- Worker polls get_next_queued() and processes jobs sequentially
- Graceful shutdown via asyncio.Event
- Blocking transcription runs in thread pool via asyncio.to_thread()
- Job state transitions: QUEUED -> PROCESSING -> COMPLETED|ERROR
- Error handling: failed jobs get ERROR status with message
- All tests pass (new and existing)
</success_criteria>

<output>
After completion, create `.planning/phases/03-background-worker/03-01-SUMMARY.md`
</output>
